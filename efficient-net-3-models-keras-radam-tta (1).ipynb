{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.models import Model, Input, load_model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras import backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import scipy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def process_csv(dataframe: pd.DataFrame, image_column_name: str,\n",
    "                label_column_name: str,\n",
    "                folder_with_images: str) -> pd.DataFrame:\n",
    "    \"\"\"This function process Pandas DataFrame, which contains image filenames\n",
    "    and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        dataframe: Pandas DataFrame object. It should consist of 2 columns\n",
    "        image_column_name: The name of the column containing the image\n",
    "            filenames\n",
    "        label_column_name: The name of the column containing the image\n",
    "            labels\n",
    "        folder_with_images: Folder with images\n",
    "\n",
    "    Returns:\n",
    "        dataframe: processed DataFrame with full paths to images\n",
    "    \"\"\"\n",
    "    dataframe[image_column_name] = dataframe[image_column_name].apply(\n",
    "        lambda x: f\"{folder_with_images}{x}.png\")\n",
    "    dataframe[label_column_name] = dataframe[label_column_name].astype('str')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.01,\n",
    "                                   zoom_range=[0.9, 1.25],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='reflect',\n",
    "                                   data_format='channels_last',\n",
    "                                   brightness_range=[0.5, 1.5],\n",
    "                                   validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2564 validated image filenames belonging to 5 classes.\n",
      "Found 1098 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/train.csv\")\n",
    "train_csv = process_csv(\n",
    "    dataframe=train_csv,\n",
    "    image_column_name=\"id_code\",\n",
    "    label_column_name=\"diagnosis\",\n",
    "    folder_with_images=\"/kaggle/input/aptos2019-blindness-detection/train_images/\")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_csv, x_col=\"id_code\", y_col=\"diagnosis\", subset=\"training\",\n",
    "    batch_size=8, target_size=(299, 299))\n",
    "val_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_csv, x_col=\"id_code\", y_col=\"diagnosis\",\n",
    "    subset=\"validation\", batch_size=8, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, lr, beta1=0.9, beta2=0.99, decay=0, **kwargs):\n",
    "        super(RAdam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.lr = K.variable(lr)\n",
    "            self._beta1 = K.variable(beta1, dtype=\"float32\")\n",
    "            self._beta2 = K.variable(beta2, dtype=\"float32\")\n",
    "            self._max_sma_length = 2 / (1 - self._beta2)\n",
    "            self._iterations = K.variable(0)\n",
    "            self._decay = K.variable(decay)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self._iterations, 1)]\n",
    "        first_moments = [K.zeros(K.int_shape(p), dtype=K.dtype(p))\n",
    "                         for (i, p) in enumerate(params)]\n",
    "        second_moments = [K.zeros(K.int_shape(p), dtype=K.dtype(p))\n",
    "                          for (i, p) in enumerate(params)]\n",
    "\n",
    "        self.weights = [self._iterations] + first_moments + second_moments\n",
    "        bias_corrected_beta1 = K.pow(self._beta1, self._iterations)\n",
    "        bias_corrected_beta2 = K.pow(self._beta2, self._iterations)\n",
    "        for i, (curr_params, curr_grads) in enumerate(zip(params, grads)):\n",
    "            # Updating moving moments\n",
    "\n",
    "            new_first_moment = self._beta1 * first_moments[i] + (\n",
    "                    1 - self._beta1) * curr_grads\n",
    "            new_second_moment = self._beta2 * second_moments[i] + (\n",
    "                    1 - self._beta2) * K.square(curr_grads)\n",
    "            self.updates.append(K.update(first_moments[i],\n",
    "                                         new_first_moment))\n",
    "            self.updates.append(K.update(second_moments[i],\n",
    "                                         new_second_moment))\n",
    "\n",
    "            # Computing length of approximated SMA\n",
    "\n",
    "            bias_corrected_moving_average = new_first_moment / (\n",
    "                    1 - bias_corrected_beta1)\n",
    "            sma_length = self._max_sma_length - 2 * (\n",
    "                    self._iterations * bias_corrected_beta2) / (\n",
    "                                 1 - bias_corrected_beta2)\n",
    "\n",
    "            # Bias correction\n",
    "\n",
    "            variance_rectification_term = K.sqrt(\n",
    "                self._max_sma_length * (sma_length - 4) * (sma_length - 2) / (\n",
    "                        sma_length * (self._max_sma_length - 4) *\n",
    "                        (self._max_sma_length - 2) + K.epsilon()))\n",
    "            resulting_parameters = K.switch(\n",
    "                sma_length > 5, variance_rectification_term *\n",
    "                bias_corrected_moving_average / K.sqrt(\n",
    "                    K.epsilon() + new_second_moment / (1 -\n",
    "                                                       bias_corrected_beta2)),\n",
    "                bias_corrected_moving_average)\n",
    "            resulting_parameters = curr_params - self.lr * resulting_parameters\n",
    "            self.updates.append(K.update(curr_params, resulting_parameters))\n",
    "        if self._decay != 0:\n",
    "            new_lr = self.lr * (1. / (1. + self._decay * K.cast(\n",
    "                self._iterations, K.dtype(self._decay))))\n",
    "            self.updates.append(K.update(self.lr, new_lr))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"lr\": float(K.get_value(self.lr)),\n",
    "            \"beta1\": float(K.get_value(self._beta1)),\n",
    "            \"beta2\": float(K.get_value(self._beta2)),\n",
    "        }\n",
    "        base_config = super(RAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Average, Input\n",
    "\n",
    "sys.path.append(os.path.abspath('../input/kerasefficientnetsmaster/keras-efficientnets-master/keras-efficientnets-master/'))\n",
    "from keras_efficientnets import EfficientNetB7\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    input_tensor = Input((299, 299, 3))\n",
    "    outputs = []\n",
    "    \n",
    "    effnet = EfficientNetB7(input_shape=(299,299,3),\n",
    "                        weights=sys.path.append(os.path.abspath('/kaggle/input/efficientnetb0b7-keras-weights/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5')),\n",
    "                        include_top=False)\n",
    "    \n",
    "    #InceptionResNetV2_model = InceptionResNetV2(weights=None, input_shape=(299, 299, 3),include_top=False)                          \n",
    "    #InceptionResNetV2_model.load_weights(\"/kaggle/input/inceptionresnetv2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    \n",
    "    xception_model = Xception(weights=None,include_top=False,input_shape=(299,299,3))\n",
    "    xception_model.load_weights(\"/kaggle/input/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    \n",
    "    #InceptionV3_model = InceptionV3(weights=None,include_top=False,input_shape=(299, 299, 3))\n",
    "    #InceptionV3_model.load_weights(\"/kaggle/input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n",
    "    \n",
    "    pretrained_models = [\n",
    "       effnet,xception_model #xception_model #InceptionResNetV2_model #xception_model #InceptionV3_model, InceptionResNetV2_model,  #xception_model,InceptionV3_model\n",
    "    ]\n",
    "    for i, model in enumerate(pretrained_models):\n",
    "        curr_output = model(input_tensor)\n",
    "        curr_output = GlobalAveragePooling2D()(curr_output)\n",
    "        curr_output = Dense(1024, activation=\"relu\")(curr_output)\n",
    "        outputs.append(curr_output)\n",
    "    output_tensor = Average()(outputs)\n",
    "    output_tensor = Dense(5, activation=\"softmax\")(output_tensor)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-10\n",
    "end_lr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, N=5, bsize=256, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "            bsize: batch_size of the training or validation ops\n",
    "            eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom / (denom + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_kappa_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        \"best_weights.hdf5\",\n",
    "        monitor='val_kappa_loss',\n",
    "        verbose=1, save_best_only=True,\n",
    "        save_weights_only=True),\n",
    "    EarlyStopping(monitor='val_kappa_loss', patience=5),\n",
    "    reduce_lr\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "321/321 [==============================] - 748s 2s/step - loss: 0.9098 - acc: 0.6639 - kappa_loss: 17.3621 - val_loss: 2.1341 - val_acc: 0.6594 - val_kappa_loss: 15.4693\n",
      "\n",
      "Epoch 00001: val_kappa_loss improved from inf to 15.46931, saving model to best_weights.hdf5\n",
      "Epoch 2/35\n",
      "321/321 [==============================] - 602s 2s/step - loss: 0.6378 - acc: 0.7605 - kappa_loss: 9.5932 - val_loss: 1.5903 - val_acc: 0.7486 - val_kappa_loss: 9.4514\n",
      "\n",
      "Epoch 00002: val_kappa_loss improved from 15.46931 to 9.45143, saving model to best_weights.hdf5\n",
      "Epoch 3/35\n",
      "321/321 [==============================] - 640s 2s/step - loss: 0.5582 - acc: 0.7975 - kappa_loss: 8.3902 - val_loss: 1.6715 - val_acc: 0.7486 - val_kappa_loss: 10.0371\n",
      "\n",
      "Epoch 00003: val_kappa_loss did not improve from 9.45143\n",
      "Epoch 4/35\n",
      "321/321 [==============================] - 665s 2s/step - loss: 0.4731 - acc: 0.8209 - kappa_loss: 6.7053 - val_loss: 1.0133 - val_acc: 0.7240 - val_kappa_loss: 7.7216\n",
      "\n",
      "Epoch 00004: val_kappa_loss improved from 9.45143 to 7.72155, saving model to best_weights.hdf5\n",
      "Epoch 5/35\n",
      "321/321 [==============================] - 695s 2s/step - loss: 0.4337 - acc: 0.8400 - kappa_loss: 5.7927 - val_loss: 1.4913 - val_acc: 0.7514 - val_kappa_loss: 10.6019\n",
      "\n",
      "Epoch 00005: val_kappa_loss did not improve from 7.72155\n",
      "Epoch 6/35\n",
      "321/321 [==============================] - 619s 2s/step - loss: 0.3750 - acc: 0.8629 - kappa_loss: 5.2688 - val_loss: 1.1289 - val_acc: 0.7878 - val_kappa_loss: 7.1955\n",
      "\n",
      "Epoch 00006: val_kappa_loss improved from 7.72155 to 7.19547, saving model to best_weights.hdf5\n",
      "Epoch 7/35\n",
      "321/321 [==============================] - 598s 2s/step - loss: 0.3521 - acc: 0.8727 - kappa_loss: 4.4629 - val_loss: 1.2904 - val_acc: 0.7432 - val_kappa_loss: 7.7953\n",
      "\n",
      "Epoch 00007: val_kappa_loss did not improve from 7.19547\n",
      "Epoch 8/35\n",
      "321/321 [==============================] - 606s 2s/step - loss: 0.3298 - acc: 0.8808 - kappa_loss: 4.4153 - val_loss: 1.0934 - val_acc: 0.7659 - val_kappa_loss: 8.1833\n",
      "\n",
      "Epoch 00008: val_kappa_loss did not improve from 7.19547\n",
      "Epoch 9/35\n",
      "321/321 [==============================] - 612s 2s/step - loss: 0.3143 - acc: 0.8871 - kappa_loss: 4.3201 - val_loss: 1.6515 - val_acc: 0.7568 - val_kappa_loss: 9.0121\n",
      "\n",
      "Epoch 00009: val_kappa_loss did not improve from 7.19547\n",
      "Epoch 10/35\n",
      "321/321 [==============================] - 604s 2s/step - loss: 0.2709 - acc: 0.8995 - kappa_loss: 3.6954 - val_loss: 1.7381 - val_acc: 0.7304 - val_kappa_loss: 9.0380\n",
      "\n",
      "Epoch 00010: val_kappa_loss did not improve from 7.19547\n",
      "Epoch 11/35\n",
      "321/321 [==============================] - 598s 2s/step - loss: 0.2552 - acc: 0.9030 - kappa_loss: 3.4478 - val_loss: 1.4371 - val_acc: 0.7413 - val_kappa_loss: 7.6770\n",
      "\n",
      "Epoch 00011: val_kappa_loss did not improve from 7.19547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa07e528518>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer=RAdam(1e-4),\n",
    "              loss=\"categorical_crossentropy\", metrics=[\"accuracy\", kappa_loss])\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=len(train_generator),\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=len(val_generator),\n",
    "                    epochs=35,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_augmentation(image, network_model):\n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    all_images = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    flip_horizontal_image = np.expand_dims(datagen.apply_transform(\n",
    "        x=image, transform_parameters={\"flip_horizontal\": True}), axis=0)\n",
    "    all_images = np.append(all_images, flip_horizontal_image, axis=0)\n",
    "    \n",
    "    flip_vertical_image = np.expand_dims(datagen.apply_transform(\n",
    "        x=image, transform_parameters={\"flip_vertical\": True}), axis=0)\n",
    "    all_images = np.append(all_images, flip_vertical_image, axis=0)\n",
    "    \n",
    "    rotated_image = np.expand_dims(datagen.apply_transform(\n",
    "        x=image, transform_parameters={\"theta\": randint(0, 15)}), axis=0)\n",
    "    all_images = np.append(all_images, rotated_image, axis=0)\n",
    "    \n",
    "    prediction = int(np.argmax(np.mean(network_model.predict(all_images), axis=0)))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(\"/kaggle/input/aptos2019-blindness-detection/test.csv\")\n",
    "predicted_csv = pd.DataFrame(columns=[\"id_code\", \"diagnosis\"])\n",
    "\n",
    "for id_code in test_csv[\"id_code\"]:\n",
    "    filename = f\"/kaggle/input/aptos2019-blindness-detection/test_images/{id_code}.png\"\n",
    "    img = imread(filename)\n",
    "    img = cv2.resize(img, dsize=(299, 299) , interpolation=cv2.INTER_CUBIC)\n",
    "    prediction = test_time_augmentation(img, model)\n",
    "    predicted_csv = predicted_csv.append(\n",
    "        {'id_code':id_code ,\"diagnosis\": prediction}, ignore_index=True)\n",
    "\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(predicted_csv.to_csv(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
